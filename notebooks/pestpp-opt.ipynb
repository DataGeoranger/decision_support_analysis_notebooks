{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PESTPP-OPT\n",
    "\n",
    "In this notebook we will setup and solve a mgmt optimization problem around how much groundwater can be pumped while maintaining sw-gw exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "plt.rcParams['font.size']=12\n",
    "import flopy\n",
    "import pyemu\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPER IMPORTANT: SET HOW MANY PARALLEL WORKERS TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = \"template_history\"\n",
    "m_d = \"master_opt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = flopy.modflow.Modflow.load(\"freyberg.nam\",model_ws=t_d,check=False,forgive=False)\n",
    "# plot some model attributes\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "ax = plt.subplot(111,aspect=\"equal\")\n",
    "mm = flopy.plot.PlotMapView(model=m)\n",
    "mm.plot_grid()\n",
    "mm.plot_ibound()\n",
    "mm.plot_bc('SFR')\n",
    "mm.plot_bc(\"GHB\")\n",
    "ax = mm.ax\n",
    "#m.wel.stress_period_data.plot(ax=ax,mflay=2)\n",
    "\n",
    "# plot obs locations\n",
    "obs = pd.read_csv(os.path.join(\"..\",\"base_model_files\",\"obs_loc.csv\"))\n",
    "                  \n",
    "obs_x = [m.sr.xcentergrid[r-1,c-1] for r,c in obs.loc[:,[\"row\",\"col\"]].values]\n",
    "obs_y = [m.sr.ycentergrid[r-1,c-1] for r,c in obs.loc[:,[\"row\",\"col\"]].values]\n",
    "ax.scatter(obs_x,obs_y,marker='.',label=\"water-level obs\",s=80)\n",
    "\n",
    "#plot names on the pumping well locations\n",
    "wel_data = m.wel.stress_period_data[0]\n",
    "wel_x = m.sr.xcentergrid[wel_data[\"i\"],wel_data[\"j\"]]\n",
    "wel_y = m.sr.ycentergrid[wel_data[\"i\"],wel_data[\"j\"]]\n",
    "for i,(x,y) in enumerate(zip(wel_x,wel_y)):\n",
    "    ax.scatter([x],[y],color=\"red\",marker=\"s\",s=50)\n",
    "    #ax.text(x,y,\"{0}\".format(i+1),ha=\"center\",va=\"center\")\n",
    "\n",
    "ax.set_ylabel(\"y(m)\")\n",
    "ax.set_xlabel(\"x(m)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is reset the transient pumping rates in the underlying model.  When we setup the pest interface, we extracted the existing pumping info and saved it in a directory called `list_org` for \"original list-directed input\".  What we are going to do now is copy the entire `template` directory and then rewrite the existing `WEL` package lists using the initial SS stress period rates (assuming these are long-term average rates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the template dir\n",
    "if os.path.exists(t_d+\"_opt\"):\n",
    "    shutil.rmtree(t_d+\"_opt\")\n",
    "shutil.copytree(t_d,t_d+\"_opt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the template dir we are intersted in\n",
    "t_d = t_d+\"_opt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the directory where the original list-directed input lives\n",
    "wel_dir = os.path.join(t_d,\"list_org\")\n",
    "# get just the names of the wel files in this dir\n",
    "wel_files = [f for f in os.listdir(wel_dir) if \"WEL\" in f]\n",
    "wel_files.sort()\n",
    "print(wel_files[:3])\n",
    "# the initial SS stress period wel package input\n",
    "ss_wel_file = \"WEL_0000.dat\" # the long-term avg pumping rates\n",
    "assert os.path.exists(os.path.join(wel_dir,ss_wel_file))\n",
    "\n",
    "# now replace all the other stress period wel package input\n",
    "for wel_file in wel_files[1:]:\n",
    "    shutil.copy2(os.path.join(wel_dir,ss_wel_file),os.path.join(wel_dir,wel_file))\n",
    "    \n",
    "# lets inspect the last stress period wel package input:\n",
    "pd.read_csv(os.path.join(wel_dir,wel_files[-1]),delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now let's work on manipulating the PEST interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(t_d,\"freyberg.pst\"))\n",
    "pst.write_par_summary_table(filename=\"none\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define our decision variable group and also set some `++args`.\n",
    "\n",
    "Conceptually, we are going to optimize current pumping rates to make sure we meet ecological flows during simulation year 2017.  Remember the simulation is a SS stress period followed by a series of transient stress periods.  So seeking an extraction scheme that runs through the SS period, then through 2016 and finally 2017.\n",
    "\n",
    "Define a parameter group as the devision variables (i.e. the variables that we will tune to meet the optimal condition). We will define `wellflux_k02` as the decision variable group (defined by the `++arg` called `opt_dec_var_groups`. Note in the table above that this group represents (time-invariant) flux mulipliers for each of the 6 wells. \n",
    "\n",
    "We can also define which direction we want the optimization to go using `opt_direction` as `max`. This means the objective of the optimization will be to maximize future pumping subject to the constraints we will establish below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options = {}\n",
    "#dvg = [\"welflux_k02\",\"welflux\"]\n",
    "dvg = [\"welflux_k02\"]  # time-invariant flux multiplier for each well\n",
    "pst.pestpp_options[\"opt_dec_var_groups\"] = dvg\n",
    "pst.pestpp_options[\"opt_direction\"] = \"max\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first run, we won't use chance constraints, so just fix all non-decision-variable \"parameters\".  We also need to set some realistic bounds on the `welflux` multiplier decision variables.  Finally, we need to specify a larger derivative increment for the decision varible group. For typical parameter estimation, `derinc=0.01` is often sufficient for calculating a Jacobian matrix. But, for the response matrix method of optimization, the response can be subtle requiring a greater perturbation increment. We will set it to `0.25` using some `pandas` manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "par.loc[:,\"partrans\"] = \"fixed\"\n",
    "\n",
    "temporal_wel_pars = par.loc[par.parnme.apply(lambda x: \"welflux\" in x),\"parnme\"]\n",
    "#print(temporal_wel_pars)\n",
    "#par.loc[temporal_wel_pars,\"parlbnd\"] = 0.0 \n",
    "par.loc[temporal_wel_pars,\"parval1\"] = 1.0\n",
    "\n",
    "# # dec vars\n",
    "dvg_pars = par.loc[par.pargp.apply(lambda x: x in dvg),\"parnme\"]\n",
    "par.loc[dvg_pars,\"partrans\"] = \"none\"\n",
    "par.loc[dvg_pars,\"parlbnd\"] = 0.0\n",
    "par.loc[dvg_pars,\"parubnd\"] = 3.0  # corresponds to -450 m3/d\n",
    "par.loc[dvg_pars,\"parval1\"] = 1.0\n",
    "\n",
    "par.loc[dvg_pars,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.rectify_pgroups()\n",
    "pst.parameter_groups.loc[dvg,\"inctyp\"] = \"absolute\"\n",
    "pst.parameter_groups.loc[dvg,\"inctyp\"] = \"absolute\"\n",
    "pst.parameter_groups.loc[dvg,\"derinc\"] = 0.25\n",
    "\n",
    "pst.parameter_groups.loc[dvg,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define constraints\n",
    "\n",
    "Model-based or decision-variable-related constraints are identified in `pestpp-opt` by an obs or prior information group that starts with \"less_than\" or \"greater_than\" and a weight greater than zero.  So first, we turn off all of the weights and get names for the sw-gw exchange forecasts (funny how optimization turns forecasts into constraints...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs.loc[:,\"weight\"] = 0.0\n",
    "swgw_const = obs.loc[obs.obsnme.apply(lambda x: \"fa\" in x and (\"hw\" in x or \"tw\" in x) and \"2017\" in x),\"obsnme\"]\n",
    "obs.loc[swgw_const,:].obsval.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change the obs group (`obgnme`) so that `pestpp-opt` will recognize these model outputs as constraints.  The `obsval` becomes the RHS of the constraint.  The `obsval` becomes the right-hand side of the constraint - the value that must be satified.  Here, let's make sure we are not inducing infiltration from SW to GW so we want the simulated SW-GW exchange to be less than or equal to 0.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.loc[swgw_const,\"obgnme\"] = \"less_than\"\n",
    "obs.loc[swgw_const,\"weight\"] = 1.0\n",
    "obs.loc[swgw_const,\"obsval\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define a minimum total pumping rate, otherwise this opt problem might yield a solution that doesn't give enough water for the intended usage.  We will do this through a prior information constraint since this just a sum of decision variable values - the required minimum value will the sum of current pumping multiplier parameters - this requires knowledge of what the existing rates are (listed above).  For this excersize, let's assume we need 375 $\\frac{l^3}{t} * 3$ so the prior information equation needs a rightr-hand side of 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.pst_utils.pst_config[\"prior_fieldnames\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all pumping wells are using the same rate and are of same \"water supply benefit\", we can just use a `1.0` multiplier in front of each `wel.flux` decision varialbe.  If that is not the case, then you need to set the multipliers to be more meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = pst.null_prior\n",
    "pi.loc[\"pi_1\",\"obgnme\"] = \"greater_than\"\n",
    "pi.loc[\"pi_1\",\"pilbl\"] = \"pi_1\"\n",
    "pi.loc[\"pi_1\",\"equation\"] = \" + \".join([\"1.0 * {0}\".format(d) for d in dvg_pars]) +\\\n",
    "                            \" = {0}\".format(3)\n",
    "pi.loc[\"pi_1\",\"weight\"] = 1.0\n",
    "pi.equation[\"pi_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.prior_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now for a risk-neutral optimization (ignoring uncertainty in constraints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that setting `noptmax=1` is equivalent to selecting Linear Programming (LP) as the optimization algorithm (thus assuming a linear response matrix). A higher value of `noptmax` runs Sequential Linear Programming (SLP) but we know this is a linear problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 1\n",
    "pst.write(os.path.join(t_d,\"freyberg_opt.pst\"))\n",
    "pyemu.os_utils.start_workers(t_d,\"pestpp-opt\",\"freyberg_opt.pst\",num_workers=num_workers,master_dir=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and inspect the response matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jco = pyemu.Jco.from_binary(os.path.join(m_d,\"freyberg_opt.1.jcb\")).to_dataframe().loc[pst.less_than_obs_constraints,:]\n",
    "jco.sort_index(inplace=True)\n",
    "jco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also load the optimal decision variable values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pyemu.pst_utils.read_parfile(os.path.join(m_d,\"freyberg_opt.1.par\"))\n",
    "print(par_df.loc[dvg_pars,\"parval1\"].sum())\n",
    "par_df.loc[dvg_pars,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of these values is the optimal objective function value - they sum to greater than the RHS of the PI constraint above. However, since these are just mulitpliers on the pumping rate, this number isnt too meaningful. Instead, lets look at the residuals file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(m_d,\"freyberg_opt.pst\"),resfile=os.path.join(m_d,\"freyberg_opt.1.sim.rei\"))\n",
    "pst.res.loc[pst.nnz_obs_names,:].sort_values(by=\"modelled\",ascending=False).modelled.iloc[:10].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the \"binding constraints\" as those at 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We better also check that our prior information constraint (min total abstracted water allocation) is being met.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.res.loc[pst.prior_names,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opt under uncertainty part 1: FOSM chance constraints\n",
    "\n",
    "This is where the process of uncertainty quantification/history matching and mgmt optimizatiom meet - worlds collide! \n",
    "\n",
    "Mechanically, in PESTPP-OPT, to activate the chance constraint process, we need to specify a `risk != 0.5`.  Risk ranges from 0.001 (risk tolerant) to 0.999 (risk averse).  The larger the risk value, the more confidence we have that the (uncertain) model-based constraints are truely satisfied.  Here we will start with a risk tolerant stance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"opt_risk\"] = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the FOSM-based chance constraints, we also need to have at least one adjustable non-dec-var parameter so that we can propogate parameter uncertainty to model-based constraints (this can also be posterior FOSM is non-constraint, non-zero-weight observations are specified).  For this simple demo, lets just use the constant multiplier parameters in the prior uncertainty stance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_pars = par.loc[par.pargp.apply(lambda x: \"cn\" in x),\"parnme\"]\n",
    "cn_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "par.loc[cn_pars,\"partrans\"] = \"log\"\n",
    "pst.control_data.noptmax = 1\n",
    "pst.write(os.path.join(t_d,\"freyberg_opt_uu1.pst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we need to not only fill the response matrix (between dec vars and constraints) but we also need to fill the jacobian matrix (between parameters and constraints).  Given that we only have 6 decision variables, let's just re-populate the response matrix while also populating the Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(t_d,\"pestpp-opt\",\"freyberg_opt_uu1.pst\",num_workers=num_workers,master_dir=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we expect to see here?  What should happen to the optimal dec vars? And the constraints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(m_d,\"freyberg_opt_uu1.pst\"),resfile=os.path.join(m_d,\"freyberg_opt_uu1.1.sim.rei\"))\n",
    "pst.res.loc[pst.nnz_obs_names,:].sort_values(by=\"modelled\",ascending=False).modelled.iloc[:20].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait - was the constraint that sw-gw flux had to be less than 0?  How do we have some many constraints greater than 0???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pyemu.pst_utils.read_parfile(os.path.join(m_d,\"freyberg_opt_uu1.1.par\"))\n",
    "print(par_df.loc[dvg_pars,\"parval1\"].sum())\n",
    "par_df.loc[dvg_pars,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see how taking a risk tolerant stance allows for more pumping but that we have only a 40% chance of actually satifying the sw-gw constraints (see how the model simulated value is actually in violation of the constraints).  Lets check the residuals that include the FOSM-based chance constraint shift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pyemu.pst_utils.read_resfile(os.path.join(m_d,\"freyberg_opt_uu1.1.sim+chance.rei\")).loc[pst.nnz_obs_names,:]\n",
    "res_df.sort_values(by=\"modelled\",ascending=False).modelled.iloc[:10].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what is happening?  Because we were risk tolerant, we actually subtracted some sw-gw flux from the values from MODFLOW - effectively acounting for additional sw-gw that wasnt acutally simulated.  This flux is in the uncertainty in the simulated sw-gw flux.  And since we were \"aggresive\" with our risk-stance, we can exploit this uncertainty to extract additional groundwater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opt under uncertainty part 2: ensemble-based chance constraints\n",
    "\n",
    "PESTPP-OPT can also skip the FOSM calculations if users specify model-based constraint weights as standard deviations (i.e. uncertainty in the forecasts/constraints).  These can be derived from our existing ensembles (oh snap!).  This removes the assumed linear relation between parameters and constaints (e.g. forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = pd.read_csv(os.path.join(\"master_prior_sweep\",\"sweep_out.csv\"),index_col=0)\n",
    "obs_df = obs_df.loc[obs_df.failed_flag==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_std = obs_df.std().loc[pst.nnz_obs_names]\n",
    "pr_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we can also skip the FOSM calcs within `PESTPP-OPT` using weights as per previous FOSM standard deviation calcs, not just ensemble-based standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.loc[pst.nnz_obs_names,\"weight\"] = pr_std.loc[pst.nnz_obs_names]\n",
    "pst.pestpp_options[\"opt_std_weights\"] = True\n",
    "pst.write(os.path.join(t_d,\"freyberg_opt_uu2.pst\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.start_workers(t_d,\"pestpp-opt\",\"freyberg_opt_uu2.pst\",num_workers=num_workers,master_dir=m_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pyemu.pst_utils.read_parfile(os.path.join(m_d,\"freyberg_opt_uu2.1.par\"))\n",
    "print(par_df.loc[dvg_pars,\"parval1\"].sum())\n",
    "par_df.loc[dvg_pars,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the objective function higher when we use the ensemble-based constraint uncertainty compared to the FOSM constraint uncertainty?  The prior FOSM uncertainty estimates are larger than the ensemble-based estimates..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super secret mode for `LP`\n",
    "\n",
    "It turns out, if the opt problem is truely linear, we can reuse results of a previous PESTPP-OPT run to modify lots of the pieces of the optimization problem and resolve the optimization problem without running the model even once!  WAT!? \n",
    "\n",
    "As long as the same decision variables are relates to the same responses, and we can fairly assume that the response matrix that relates the decision variables to the constraints is linear, then the response matrix doesn't change even if things like bounds and risk level change. We just need `pestpp-opt` to read in the response matrix (which is stored with the same format as a Jacobian (`jcb`)) and the residuals (`rei`). \n",
    "\n",
    "This is done by specifying some additional `++args` (and copying some files around)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy2(os.path.join(m_d,\"freyberg_opt_uu2.1.jcb\"),os.path.join(m_d,\"restart.jcb\"))\n",
    "shutil.copy2(os.path.join(m_d,\"freyberg_opt_uu2.1.jcb.rei\"),os.path.join(m_d,\"restart.rei\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have copied over the necessary files, we set a few `++args`:  \n",
    "* `base_jacobian`: this instructs `pestpp-opt` to read in the existing response matrix\n",
    "* `hotstart_resfile`: this instructs `pestpp-opt` to use the residuals we already have\n",
    "* `opt_skip_final`: this waives the usual practice of running the model once with optimal parameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which runs do each of these skip specifically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"base_jacobian\"] = \"restart.jcb\"\n",
    "pst.pestpp_options[\"hotstart_resfile\"] = \"restart.rei\"\n",
    "pst.pestpp_options[\"opt_skip_final\"] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_data.loc[:,\"partrans\"] = \"fixed\"\n",
    "pst.parameter_data.loc[dvg_pars,\"partrans\"] = \"none\"\n",
    "pst.write(os.path.join(m_d,\"freyberg_opt_restart.pst\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run(\"pestpp-opt freyberg_opt_restart.pst\",cwd=m_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pyemu.pst_utils.read_parfile(os.path.join(m_d,\"freyberg_opt_restart.1.par\"))\n",
    "print(par_df.loc[dvg_pars,\"parval1\"].sum())\n",
    "par_df.loc[dvg_pars,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh snap!  that means we can do all sort of kewl optimization testing really really fast..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now let's try taking a risk averse stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options[\"opt_risk\"] = risk\n",
    "pst.write(os.path.join(m_d,\"freyberg_opt_restart.pst\"))\n",
    "pyemu.os_utils.run(\"pestpp-opt freyberg_opt_restart.pst\",cwd=m_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pyemu.pst_utils.read_parfile(os.path.join(m_d,\"freyberg_opt_restart.1.par\"))\n",
    "print(par_df.loc[dvg_pars,\"parval1\"].sum())\n",
    "par_df.loc[dvg_pars,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now lets evaluate how our OUU problem changes if we use posterior standard deviations - this is a critically important use of the uncertainty analysis from history matching...\n",
    "\n",
    "This is where all that painful history-matching finally has a real-world (and valid) purpose: to reduce the uncertainty in the constraints for a decision-support optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = pd.read_csv(os.path.join(\"master_ies\",\"freyberg_ies.3.obs.csv\"),index_col=0)\n",
    "#obs_df = pd.read_csv(os.path.join(\"master_glm_run\",\"freyberg_pp.post.obsen.csv\"),index_col=0)\n",
    "#df = df=pd.read_csv(os.path.join(\"master_glm\",\"freyberg_pp.post.obsen.csv\"),index_col=0)\n",
    "#obs_df = pyemu.ObservationEnsemble.from_dataframe(pst=pst,df=df)\n",
    "#obs_df = obs_df.loc[obs_df.phi_vector.sort_values().index[:20],:] \n",
    "pt_std = obs_df.std().loc[pst.nnz_obs_names]\n",
    "obs_df.std().loc[pst.nnz_obs_names]\n",
    "#obs_df.max().loc[pst.nnz_obs_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much lower is the posterior standard deviations as compared to the prior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"prior\":pr_std,\"posterior\":pt_std})\n",
    "df.sort_index(inplace=True)\n",
    "df.plot(kind=\"bar\",figsize=(20,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that the chance constraints (which express the important model input uncertainty propogated to the forecast/constraints) is significantly lower, meaning uncertainty has less impact (read \"value\") in the optimization objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.loc[pst.nnz_obs_names,\"weight\"] = pt_std.loc[pst.nnz_obs_names]\n",
    "pst.observation_data.loc[pst.nnz_obs_names,\"weight\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(m_d,\"freyberg_opt_restart.pst\"))\n",
    "pyemu.os_utils.run(\"pestpp-opt freyberg_opt_restart.pst\",cwd=m_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pyemu.pst_utils.read_parfile(os.path.join(m_d,\"freyberg_opt_restart.1.par\"))\n",
    "print(par_df.loc[dvg_pars,\"parval1\"].sum())\n",
    "par_df.loc[dvg_pars,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pyemu.pst_utils.read_resfile(os.path.join(m_d,\"freyberg_opt_restart.1.est+chance.rei\")).loc[pst.nnz_obs_names,:]\n",
    "df.sort_values(by=\"modelled\",ascending=False).modelled.iloc[:10].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see that historic tail water flux is the binding constraint.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some reformulation of the opt problem. Can we open up decision variable (feasibility) space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets reformulate the problem to be constrained by the total sw-gw flux across all reaches instead of splitting into headwaters and tailwaters - this relaxation makes things easier: instead of requiring that the net SW-GW flux for both the headwaters and tailwaters is 0.0, now we just want to make sure the net SW-GW flux over the whole SW network is 0.0.  Good thing we have added the list file budget components to the control file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(m_d,\"freyberg_opt_restart.pst\"))\n",
    "obs = pst.observation_data\n",
    "obs.loc[pst.nnz_obs_names,\"obgnme\"] = \"sw-gw\"  # hw and tw constraints from tagged \"less_than\"\n",
    "obs.loc[pst.nnz_obs_names,\"weight\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_swgw = obs.loc[obs.obsnme.apply(lambda x: x.startswith(\"flx_stream_\") and \"2017\" in x),\"obsnme\"]\n",
    "tot_swgw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.loc[tot_swgw,\"obgnme\"] = \"less_than\"\n",
    "obs.loc[tot_swgw,\"weight\"] = 1.0\n",
    "obs.loc[tot_swgw,\"weight\"] = obs_df.std().loc[pst.nnz_obs_names]\n",
    "obs.loc[tot_swgw,\"obsval\"] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your understanding:  we just reformulated the mgmt optimization problem to use the total sw-gw flux along all reaches instead of dividing the constraints into a headwater and tailwater reach grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df.std().loc[pst.nnz_obs_names].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it - we just transformed the optimization problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## risk sweeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we really want to find the most risk averse stance that is still feasible we will run a sweep of risk values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_dfs = []\n",
    "res_dfs = []\n",
    "risk_vals = np.arange(0.05,1.0,0.05)\n",
    "for risk in risk_vals:\n",
    "    #try:\n",
    "    #    os.remove(os.path.join(m_d,\"freyberg_opt_restart.1.est+fosm.rei\"))\n",
    "    #except:\n",
    "    #    pass\n",
    "   \n",
    "    pst.pestpp_options[\"opt_risk\"] = risk\n",
    "    pst.pestpp_options[\"opt_skip_final\"] = True\n",
    "    print(\"undertaking evaluation for risk value: {0:.2f}\".format(risk))\n",
    "    pst.write(os.path.join(m_d,\"freyberg_opt_restart.pst\"))\n",
    "    pyemu.os_utils.run(\"pestpp-opt freyberg_opt_restart.pst\",cwd=m_d)\n",
    "    \n",
    "    par_df = pyemu.pst_utils.read_parfile(os.path.join(m_d,\"freyberg_opt_restart.1.par\"))\n",
    "    par_df = par_df.loc[dvg_pars,:]\n",
    "    #when the solution is infeasible, pestpp-opt writes extreme negative values \n",
    "    # to the par file:\n",
    "    if par_df.parval1.sum() < 6.0: \n",
    "        print(\"infeasible at risk {0:.2f}\".format(risk))\n",
    "        break\n",
    "    \n",
    "    res_df = pyemu.pst_utils.read_resfile(os.path.join(m_d,\"freyberg_opt_restart.1.est+chance.rei\"))\n",
    "    res_df = res_df.loc[pst.nnz_obs_names,:]\n",
    "    res_dfs.append(res_df.modelled)\n",
    "    par_dfs.append(par_df.parval1)\n",
    "\n",
    "# process the dec var and constraint dataframes for plotting\n",
    "risk_vals = risk_vals[:len(par_dfs)]\n",
    "par_df = pd.concat(par_dfs,axis=1).T\n",
    "par_df.index = risk_vals\n",
    "par_df.index = par_df.index.map(lambda x: \"{0:0.3f}\".format(x))\n",
    "res_df = pd.concat(res_dfs,axis=1).T\n",
    "res_df.index = risk_vals\n",
    "res_df.index = res_df.index.map(lambda x: \"{0:0.3f}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"m\",\"c\",\"g\",\"r\",\"b\",\"orange\"]\n",
    "fig, axes = plt.subplots(2,1,figsize=(20,15))\n",
    "par_df.plot(kind=\"bar\",ax=axes[0],alpha=0.75,color=colors).legend(bbox_to_anchor=(1.2, 0.5))\n",
    "axes[0].set_ylabel(\"individual pumping rates\")\n",
    "axes[0].set_xticklabels([])\n",
    "res_df.plot(kind=\"bar\",ax=axes[1],alpha=0.75,edgecolor=\"none\").legend(bbox_to_anchor=(1.2, 0.5))\n",
    "axes[1].plot(axes[1].get_xlim(),[-200,-200],\"r--\",lw=3)\n",
    "axes[1].set_ylabel(\"sw-gw flux\")\n",
    "axes[1].set_xlabel(\"risk\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some maps of pumping regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = flopy.modflow.Modflow.load(\"freyberg.nam\",model_ws=t_d)\n",
    "\n",
    "wf_par = pst.parameter_data.loc[dvg_pars,:].copy()\n",
    "wf_par.loc[:,\"k\"] = wf_par.parnme.apply(lambda x: int(x[2:4]))\n",
    "wf_par.loc[:,\"i\"] = wf_par.parnme.apply(lambda x: int(x[4:8]))\n",
    "wf_par.loc[:,\"j\"] = wf_par.parnme.apply(lambda x: int(x[8:]))\n",
    "wf_par.loc[:,\"x\"] = wf_par.apply(lambda x: m.sr.xcentergrid[x.i,x.j],axis=1)\n",
    "wf_par.loc[:,\"y\"] = wf_par.apply(lambda x: m.sr.ycentergrid[x.i,x.j],axis=1)\n",
    "\n",
    "ib = m.bas6.ibound[0].array\n",
    "ib = np.ma.masked_where(ib!=0,ib)\n",
    "fig,axes = plt.subplots(5,int(np.ceil(par_df.shape[0]/5)),figsize=(12,12))\n",
    "axes = axes.flatten()\n",
    "for risk,ax in zip(par_df.index,axes):\n",
    "    ax.set_aspect(\"equal\")\n",
    "    #ax = plt.subplot(111,aspect=\"equal\") \n",
    "    ax.imshow(ib,extent=m.sr.get_extent())\n",
    "    ax.scatter(wf_par.x,wf_par.y,s=par_df.loc[risk,wf_par.parnme].values*50,c=colors)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(risk)\n",
    "    \n",
    "for i in range(par_df.shape[0],axes.shape[0]):\n",
    "    ax = axes[i]\n",
    "    ax.axis(\"off\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How slick was that!  no more model runs needed and yet we transformed the OUU problem (by swapping constraints) and solved for a much more risk averse stance!  Just to make sure, lets run the model with the most risk-averse decision variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if risk_vals[-1] == 0.5:\n",
    "    risk_vals = risk_vals[:-1]\n",
    "pst.pestpp_options[\"opt_risk\"] = risk_vals[-1]\n",
    "pst.pestpp_options[\"opt_skip_final\"] = False\n",
    "pst.write(os.path.join(m_d,\"freyberg_opt_restart.pst\"))\n",
    "pyemu.os_utils.run(\"pestpp-opt freyberg_opt_restart.pst\",cwd=m_d)\n",
    "# load the simulated outputs plus the FOSM chance constraint offsets:\n",
    "res_df = pyemu.pst_utils.read_resfile(os.path.join(m_d,\"freyberg_opt_restart.1.sim+chance.res\"))\n",
    "res_df = res_df.loc[pst.nnz_obs_names,:]\n",
    "res_df.sort_values(by=\"modelled\",ascending=False).modelled.iloc[:10].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the actual model simulated outputs\n",
    "res_df_sim = pyemu.pst_utils.read_resfile(os.path.join(m_d,\"freyberg_opt_restart.1.sim.rei\"))\n",
    "res_df_sim = res_df_sim.loc[pst.nnz_obs_names,:]\n",
    "ax = pd.DataFrame({\"sim\":res_df_sim.modelled,\"sim+fosm\":res_df.modelled}).plot(kind=\"bar\",figsize=(20,10))\n",
    "ax.plot(ax.get_xlim(),[0,0],\"r--\",lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the cost of uncertainty - we have to simulate a greater flux from gw to sw to make sure (e.g. be risk averse) that the flux from  gw to sw is actually 0 m3/day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# FINALLY!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see the reason for high-dimensional uncertainty quantification and history matching: to define and then reduce (through data assimilation) the uncertainty in the model-based constraints (e.g. sw-gw forecasts) so that we can find a more risk-averse management solution - we can use a model to identify an optimal pumping scheme to provide the volume of water needed for water supply, agriculture, etc. but also provide assurances (at the given confidence) that ecological flows will be maintained under both current conditions and in the event of an extreme 1-year drought.  BOOM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
