{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Regression using a Polynomial and  least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import scipy.optimize as spo\n",
    "import matplotlib.pyplot as plt\n",
    "import regression_helper as rh\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First cook up some data\n",
    "\n",
    "Set a range of x-values, then make a \"true\" set of y-values using a second degree polynomial (e.g. `best_degree=2`).\n",
    "\n",
    "The general form of the polynomial is: \n",
    "## $y=ax^2 + bx + c$\n",
    "\n",
    "We can set $c=0$ for our purposes so we have two free parameters: $a$ and $b$\n",
    "\n",
    "Also add noise to the \"true\" observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplot, x, y_data, poly_func, polypars = rh.data_cooker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to define a couple functions\n",
    "\n",
    "The `parabola` function calculates the equation for a parabola. Note that `c` is skipped since we set it to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parabola(a,b,x_vec):\n",
    "    y = [a*x**2 + b*x for x in x_vec]\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `errfun` function calculates a vector of the differences between a vector of data (`y`) and the parabola function estimates at the same locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errfun(pars,x,y):\n",
    "    return y-parabola(*pars,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data and the true model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh.plot_truth(xplot,x,y_data, poly_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a function to calculate the sum of squared errors\n",
    "\n",
    "This function calculates the sum of squared errors between observations and modeled equivalents. It is more general than `errfun` above which only works with the parabola function:\n",
    "\n",
    "## $SSE=\\Phi=\\sum_{i=1}^{NPAR}\\left(y_{i}-m\\left(x_{i}\\right)\\right)^{2}$\n",
    "where: \n",
    " ## * $SSE$ is sum of squared errors\n",
    " ## * $y_i$ is the $i^{th}$ observation\n",
    " ## * $m\\left(x_i\\right)$ is the modeled equivalent to the $i^{th}$ observation\n",
    " \n",
    "In vector notation, this is expressed as:\n",
    "## $\\Phi=\\left(\\mathbf{y}-\\mathbf{m}\\right)^{T}\\left(\\mathbf{y}-\\mathbf{m}\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squared_errors(y,m):\n",
    "    y = np.atleast_1d(y)\n",
    "    m = np.atleast_1d(m)\n",
    "    sse = np.dot((y-m).T,(y-m))\n",
    "    return sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use $\\Phi$ to evaluate the response surface\n",
    "\n",
    "## We can look at the response to change is parameters\n",
    "\n",
    "### First as linear plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = rh.plot_sse(polypars, x, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polypars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or, since `python` is slick, we can make a contour plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B, SSE_AB = rh.contour_sse(a, b, x, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or, even slicker, we can see it in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh.surface_sse(a, b, A, B, SSE_AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a polynomial function\n",
    "\n",
    "## Now fit a function assuming it will be a polynomial of the same degree (e.g. `best_degree`) as was used to generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the polynomial and then plot the resulting function\n",
    "sol = spo.least_squares(errfun,[-2,2],args=(x,y_data))\n",
    "y_fit_pars_best = [*sol.x,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the Jacobian matrix---gradients of parameters wrt. observations\n",
    "\n",
    "## For each parameter-observation combination, we can see how much the observation value changes due to a small change in the parameter. If $y$ are the observations and $x$ are the parameters, the equation for the $i^th$ observation with respect to the $j^th$ parameter is:  \n",
    "## $\\frac{\\partial y_i}{\\partial x_j}$\n",
    "## This can be approximated by finite differences as :  \n",
    "## $\\frac{\\partial y_i}{\\partial x_j}~\\frac{y\\left(x+\\Delta x \\right)-y\\left(x\\right)}{\\Delta x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh.plot_jacobian(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can create a function based on the best fit parameters and use it for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_fit_best = np.poly1d(y_fit_pars_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh.plot_best_fit(x, poly_func, func_fit_best, y_data, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('True parameters are:              a={0:.4f}, b={1:.4f}, c={2}'.format(*polypars))\n",
    "print('The best-estimate parameters are: a={0:.4f}, b={1:.4f}, c={2}'.format(*y_fit_pars_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh.plot_prediction(x, y_data, poly_func, func_fit_best)\n",
    "#x_predlocations = np.linspace(x[0]-range_x*0.2, x_pred, 1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see what happens over a range of polynomial values\n",
    "We can change the `offset` parameter to plot further outside the data range\n",
    "\n",
    "We can also change the `extra_degrees` variable to plot curves of higher polynomial degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_x = (x[-1]-x[0])\n",
    "x_pred = x[-1]+range_x*0.21\n",
    "x_predlocations = np.linspace(x[0]-range_x*0.2, x_pred, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit_pars = np.polyfit(x,y_data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interact(rh.plot_poly, cdegree=widgets.IntSlider(min=1,max=30,step=1,value=3),\n",
    "                 y_fit_pars_best=widgets.fixed(y_fit_pars_best),\n",
    "                 poly_func=widgets.fixed(poly_func),\n",
    "                 x=widgets.fixed(x),\n",
    "                 x_pred=widgets.fixed(x_pred),\n",
    "                 x_predlocations=widgets.fixed(x_predlocations),\n",
    "                 y_data=widgets.fixed(y_data), \n",
    "                 y_fit_pars=widgets.fixed(y_fit_pars));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_range=list(range(1,8))\n",
    "best_degree=2\n",
    "all_datafit, all_predfit = rh.fit_all_curves(x,x_pred,y_data, poly_func, degree_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh.plot_error_tradeoff(all_datafit, all_predfit, degree_range, best_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rh.plot_error_tradeoff_fine(all_datafit, all_predfit, degree_range, best_degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
