{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyemu\n",
    "import os, shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import geostat_helpers as gh\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import normaltest\n",
    "from scipy.stats import norm\n",
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geostatistics \n",
    "### Some definitions from Geoff Bohling http://people.ku.edu/~gbohling/cpe940/Variograms.pdf\n",
    "> ## “Geostatistics: study of phenomena that vary in space and/or time”\n",
    "(Deutsch, 2002)\n",
    "\n",
    "\n",
    "> ## “Geostatistics can be regarded as a collection of numerical techniques that deal with the characterization of spatial attributes, employing primarily random models in a manner similar to the way in which time series analysis characterizes temporal data.”\n",
    "(Olea, 1999)\n",
    "\n",
    "\n",
    "> ## “Geostatistics offers a way of describing the spatial continuity of natural phenomena and provides adaptations of classical regression techniques to take advantage of this continuity.” \n",
    "(Isaaks and Srivastava, 1989)\n",
    "\n",
    "\n",
    "> ## Geostatistics deals with spatially _autocorrelated_ data.\n",
    "\n",
    "> ## Autocorrelation: correlation between elements of a series and others from the same series separated from them by a given interval. \n",
    "(Oxford American Dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Concepts\n",
    "\n",
    "\n",
    "## 1. Variogram modeling -- a way to characterize spatial correlation\n",
    "## 2. Kriging -- a best linear unbiased estimate (BLUE) for interpolation with minimum variance. There are several flavors - we will focus on Ordinary Kriging\n",
    "## 3. Stochastic Simulation -- http://petrowiki.org/Geostatistical_conditional_simulation\n",
    "## 4. Beyond this multi-Gaussian approach focused on the relationships among pairs of points, there is _multiple point geostatistics_ as well using training images and more complex shapes\n",
    "\n",
    "# These concepts each build on each other. We will briefly touch on the first two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## let's cook up a quick random field and explore the spatial structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y,Z,v,gs,sample_df = gh.data_cooker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretend (key word!) that this is a hydraulic conductivity field\n",
    "\n",
    "## Any _autocorrelation_ here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh.grid_plot(X,Y,Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Of course, we would typically only know the values at a few points \n",
    "## (and probably not perfectly)\n",
    "## N.B. --> The default number of samples used here is 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh.field_scatterplot(sample_df.x,sample_df.y,sample_df.z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geostatistics is based on a couple main assumptions:\n",
    "   ## 1. The values are second order stationary (the mean and variance are relatively constant) \n",
    "   ## 2. The values are multi-Gaussian (e.g. normally distributed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are the raw data normally distributed?\n",
    "\n",
    "### Spoiler alert - this field was generated using a variogram!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Z.ravel(), bins=50, density=True)\n",
    "x=np.linspace(min(Z.ravel()),max(Z.ravel()),100)\n",
    "plt.plot(x,sps.norm.pdf(x, np.mean(Z),np.std(Z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaltest(Z.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How about our subsample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sample_df.z, bins=50, density=True)\n",
    "x=np.linspace(min(Z.ravel()),max(Z.ravel()),100)\n",
    "plt.plot(x,sps.norm.pdf(x, np.mean(sample_df.z),np.std(sample_df.z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaltest(sample_df.z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That was a pretty small sample....\n",
    "## Explore a bigger sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_pts=1000\n",
    "xd = np.random.uniform(0, 1000, n_sample_pts)\n",
    "yd = np.random.uniform(0, 1000, n_sample_pts)\n",
    "z = gh.sample_from_grid(X,Y,Z,xd,yd)\n",
    "plt.hist(z, bins=50, density=True)\n",
    "x=np.linspace(min(Z.ravel()),max(Z.ravel()),100)\n",
    "plt.plot(x,sps.norm.pdf(x, np.mean(z),np.std(z)))\n",
    "normaltest(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity is commendable, but we are going to violate some of these assumptions for sure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At the heart of geostatistics is some kind of model expressing the variability of properties in a field\n",
    "\n",
    "## This is a \"variogram\" and we can explore it based on the following empirical formula\n",
    "## $\\hat{\\gamma}\\left(h\\right)=\\frac{1}{2\\left(h\\right)}\\left(z\\left(x_1\\right)-z\\left(x_2\\right)\\right)^2$\n",
    "## where $x_1$ and $x_2$ are the locations of two $z$ data points separated by distance $h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we plot these up we get something called a cloud plot showing $\\hat\\gamma$ for all pairs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,gam,ax=gh.plot_empirical_variogram(sample_df.x,sample_df.y,sample_df.z,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is pretty messy, so typically it is evaluated in bins, and usually only over half the total possible distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,gam,ax=gh.plot_empirical_variogram(sample_df.x,sample_df.y,sample_df.z,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also note that this was assuming perfect observations. What if there was ~10% noise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,gam,ax=gh.plot_empirical_variogram(sample_df.x,sample_df.y,sample_df.z_noisy,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geostatistics is making the assumption that you can model the variability of this field using a variogram. The variogram is closely related to covariance. \n",
    "\n",
    "## We take advantage of a few assumptions to come up with a few functional forms that should characterize this behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pyemu` supports three variogram models\n",
    "\n",
    "## This follows the _GSLIB_ terminology\n",
    "\n",
    " ## 1. *Spherical*  \n",
    "### $\\gamma\\left(h\\right)=c\\times\\left[1.5\\frac{h}{a}-0.5\\frac{h}{a}^3\\right]$ if $h<a$\n",
    "### $\\gamma\\left(h\\right)=c$ if $h \\ge a$  \n",
    "     \n",
    " ## 2. *Exponential*  \n",
    "### $\\gamma\\left(h\\right)=c\\times\\left[1-\\exp\\left(-\\frac{h}{a}\\right)\\right]$  \n",
    "     \n",
    " ## 3. *Gaussian*  \n",
    "### $\\gamma\\left(h\\right)=c\\times\\left[1-\\exp\\left(-\\frac{h^2}{a^2}\\right)\\right]$  \n",
    "\n",
    "     \n",
    "\n",
    "### $h$ is the separation distance, and $a$ is the range. `contribution` is the variogram value at which the variogram levels off. Also called the `sill`, this value is the maximum variability between points.\n",
    "\n",
    "### The sill is reached at about $a$ for the *Spherical* model, $2a$ for the *Gaussian*, and $3a$ for the *Exponential*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do these look like?\n",
    "\n",
    "## for a consistent set of parameters:\n",
    "## a=500, c=10\n",
    "## We can use `pyemu` to setup a geostatistical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=500\n",
    "c=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a variogram object and, from that, build a geostatistical structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Spherical_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pyemu.geostats.SphVario(contribution=c, a=a)\n",
    "gs = pyemu.geostats.GeoStruct(variograms=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.plot()\n",
    "plt.plot([v.a,v.a],[0,v.contribution],'r')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q= gs.covariance_matrix(X.ravel(), Y.ravel(), names=[str(i) for i in range(len(Y.ravel()))])\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(Q.x)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Exponential_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pyemu.geostats.ExpVario(contribution=c, a=a)\n",
    "gs = pyemu.geostats.GeoStruct(variograms=v)\n",
    "gs.plot()\n",
    "plt.plot([v.a,v.a],[0,v.contribution],'r')\n",
    "plt.plot([3*v.a,3*v.a],[0,v.contribution],'r:')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q= gs.covariance_matrix(X.ravel(), Y.ravel(), names=[str(i) for i in range(len(Y.ravel()))])\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(Q.x)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Gaussian_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pyemu.geostats.GauVario(contribution=c, a=a)\n",
    "gs = pyemu.geostats.GeoStruct(variograms=v)\n",
    "gs.plot()\n",
    "plt.plot([v.a,v.a],[0,v.contribution],'r')\n",
    "plt.plot([7/4*v.a,7/4*v.a],[0,v.contribution],'r:')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q= gs.covariance_matrix(X.ravel(), Y.ravel(), names=[str(i) for i in range(len(Y.ravel()))])\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(Q.x)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If we fit an appropriate model ($\\gamma$) to the empirical variogram ($\\hat\\gamma$), we can use that structure for interpolation from sparse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,gam,ax=gh.plot_empirical_variogram(sample_df.x,sample_df.y,sample_df.z,50)\n",
    "new_c=10.0\n",
    "new_a=300.0\n",
    "\n",
    "v_fit = pyemu.geostats.ExpVario(contribution=new_c,a=new_a)\n",
    "gs_fit = pyemu.geostats.GeoStruct(variograms=v_fit)\n",
    "gs_fit.plot(ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = gs_fit.covariance_matrix(X.ravel(), Y.ravel(), names=[str(i) for i in range(len(Y.ravel()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(Q.x)\n",
    "plt.colorbar(shrink=.75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can perform Kriging to interpolate using this variogram and our \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First make an Ordinary Kriging object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pyemu.geostats.OrdinaryKrige(gs_fit,sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we need to calculate factors (we only do this once - takes a few seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfactors = k.calc_factors(X.ravel(),Y.ravel(), num_threads=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfactors = k.calc_factors(X.ravel(),Y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's easiest to think of these factors as weights on surrounding point to calculate a weighted average of the surrounding values. The weight is a function of the distance - farther points have smaller weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfactors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_interp = gh.geostat_interpolate(X,Y,k.interp_data, sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlims = [Z.min(),Z.max()]\n",
    "ax=gh.grid_plot(X,Y,Z_interp,vlims=vlims, title='reconstruction')\n",
    "ax.plot(sample_df.x,sample_df.y, 'ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh.grid_plot(X,Y,Z,vlims=vlims,title='truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=gh.grid_plot(X,Y,kfactors.err_var.values.reshape(X.shape), title='Variance of Estimate')\n",
    "ax.plot(sample_df.x,sample_df.y, 'ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=gh.grid_plot(X,Y,np.abs(Z-Z_interp), title='Actual Differences')\n",
    "ax.plot(sample_df.x,sample_df.y, 'yo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if our data were noisy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h,gam,ax=gh.plot_empirical_variogram(sample_df.x,sample_df.y,sample_df.z_noisy,30)\n",
    "new_c=15.0\n",
    "new_a=600.0\n",
    "\n",
    "# select which kind of variogram here because in reality we don't know, right?\n",
    "v_fit = pyemu.geostats.ExpVario(contribution=new_c,a=new_a)\n",
    "gs_fit = pyemu.geostats.GeoStruct(variograms=v_fit, nugget=0)\n",
    "gs_fit.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = gs_fit.covariance_matrix(X.ravel(), Y.ravel(), names=[str(i) for i in range(len(Y.ravel()))])\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(Q.x)\n",
    "plt.colorbar(shrink=.75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again make the Kriging Object and the factors and interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pyemu.geostats.OrdinaryKrige(gs_fit,sample_df)\n",
    "kfactors = k.calc_factors(X.ravel(),Y.ravel())\n",
    "Z_interp = gh.geostat_interpolate(X,Y,k.interp_data, sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=gh.grid_plot(X,Y,Z_interp,vlims=vlims, title='reconstruction')\n",
    "ax.plot(sample_df.x,sample_df.y, 'ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh.grid_plot(X,Y,Z,vlims=vlims,title='truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=gh.grid_plot(X,Y,kfactors.err_var.values.reshape(X.shape), title='Variance of Estimate')\n",
    "ax.plot(sample_df.x,sample_df.y, 'ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=gh.grid_plot(X,Y,np.abs(Z-Z_interp), title='Actual Differences')\n",
    "ax.plot(sample_df.x,sample_df.y, 'yo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral simulation\n",
    "\n",
    "Because pyemu is pure python (and because the developers are lazy), it only implments spectral simulation for grid-scale field generation.  For regular grids without anisotropy and without conditioning data (\"known\" property values), it is identical to sequential gaussian sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = pyemu.geostats.ExpVario(1.0,1)\n",
    "gs = pyemu.geostats.GeoStruct(variograms=ev)\n",
    "ss = pyemu.geostats.SpecSim2d(np.ones(100),np.ones(100),gs)\n",
    "plt.imshow(ss.draw_arrays()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = pyemu.geostats.ExpVario(1.0,5)\n",
    "gs = pyemu.geostats.GeoStruct(variograms=ev)\n",
    "ss = pyemu.geostats.SpecSim2d(np.ones(100),np.ones(100),gs)\n",
    "plt.imshow(ss.draw_arrays()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = pyemu.geostats.ExpVario(1.0,500)\n",
    "gs = pyemu.geostats.GeoStruct(variograms=ev)\n",
    "ss = pyemu.geostats.SpecSim2d(np.ones(100),np.ones(100),gs)\n",
    "plt.imshow(ss.draw_arrays()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further resources and information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # These concepts are used for pilot point interpolation in PEST\n",
    "   ## a. In the GW utilities in PEST (http://www.pesthomepage.org/Groundwater_Utilities.php) \n",
    "   ## b. The main tools are also available in `pyemu` -- we'll use that in the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A nice but outdated set of references is at: http://people.ku.edu/~gbohling/geostats/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Stanford Geostatistical Modeling Software (SGeMS: http://sgems.sourceforge.net/) is a nice GUI for geostatistical modeling, but it's not being maintained anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `R` has a package: http://rgeostats.free.fr/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some packages are under development in `python` but not very far along. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `QGIS` and `ArcGIS` have some geostatistical modeling capabilities as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Pilot Points in a `MODFLOW` model with `pyemu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = \"temp_history\"\n",
    "m = flopy.modflow.Modflow.load(\"freyberg.nam\",model_ws=t_d,check=False,forgive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.get_package_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up zones for where pilot points will be interpolated\n",
    "\n",
    "We can have pilot point networks in multiple zones. In this case, we will make a simple zone file using `IBOUND` such that all active cells are in the same interpolation zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.bas6.ibound.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We don't want pilot points or care about HK values in inactive cells, but we do need values in constant heads\n",
    "\n",
    "We are going to use a `pyemu` helper function to setup pilot points are cell centers for active cells in layer 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = 'pp_tmp'\n",
    "if os.path.exists(working_dir):\n",
    "    shutil.rmtree(working_dir)\n",
    "os.mkdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want hk pilot points in the top layer...\n",
    "prefix_dict = {1:[\"hk\"]}  # note, layer 1 is the neew layer 2 (e.g. zero-based)\n",
    "df_pp = pyemu.utils.setup_pilotpoints_grid(ml=m,prefix_dict=prefix_dict,\n",
    "                                              pp_dir=working_dir,\n",
    "                                              tpl_dir=working_dir,\n",
    "                                              every_n_cell=3)\n",
    "pp_file = os.path.join(working_dir,\"hkpp.dat\")\n",
    "assert os.path.exists(pp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to create Kriging factors and regularization inputs\n",
    "Following the guidelines in _Approaches to Highly Parameterized Inversion: Pilot-Point Theory, Guidelines, and Research Directions_ https://pubs.usgs.gov/sir/2010/5168/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's create ``variogram`` and ``GeoStruct`` objects.  \n",
    "\n",
    "These describe how HK varies spatailly, remember?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pyemu.geostats.ExpVario(contribution=1.0,a=2500)\n",
    "gs = pyemu.geostats.GeoStruct(variograms=v,nugget=0.0)\n",
    "ax = gs.plot()\n",
    "ax.grid()\n",
    "ax.set_ylim(0,2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get an ``OrdinaryKrige`` object, which needs the ``GeoStruct`` as well as the x, y, and name of the pilot point locations (which happens to be in that really cool ``df_pp`` instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = pyemu.geostats.OrdinaryKrige(gs,df_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the ``OrdinaryKrige`` is created, we need to calculate the geostatistical interpolation factors for each model cell.  We do this with the ``.calc_factors_grid()`` method: it needs to know about the model's spatial orientation and also accepts some optional arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ok.calc_factors_grid(m.sr,var_filename=os.path.join(working_dir,'kriging.var'),\n",
    "                          minpts_interp=1,maxpts_interp=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the really cool things about geostatistics is that it gives you both the interpolation (factors), but also gives you the uncertainty in the areas between control (pilot) points.  Above, we wrote this uncertainty information to an array that has the same rows and cols as the model grid - this array is very useful for understanding the function of the variogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the kriging variance\n",
    "arr_var = np.loadtxt(os.path.join(working_dir,'kriging.var'))\n",
    "ax = plt.subplot(111,aspect=\"equal\")\n",
    "p = ax.imshow(arr_var,extent=m.sr.get_extent(),alpha=0.25)\n",
    "plt.colorbar(p)\n",
    "ax.scatter(df_pp.x,df_pp.y,marker='.',s=4,color='r')\n",
    "ax.set_title('Kriging Variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that at the pilot point locations (red dots), the uncertainty in the geostats is minimal...as expected. The call to ``.calc_factors_grid()`` also returns a ``DataFrame`` which has useful info - lets look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is one row for each model cell, and for each row, we see the distance, names, and weight for the \"nearby\" pilot points.  The interpolated value for cells that have a pilot point at their center only need one weight - 1.0 - and one pilot point.  Other cells are weighted combinations of pilot points.  Is this clear?  \n",
    "\n",
    "Now we need to save the factors (weights) to a special file that we will use later to quickly generate a new HK array from a set of pilot point values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok.to_grid_factors_file(pp_file+\".fac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for demo purposes, lets generate ``random`` pilot point values and run them through the factors to see what the ``hk`` array looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random values\n",
    "df_pp.loc[:,\"parval1\"] = np.random.random(df_pp.shape[0])*25\n",
    "# save a pilot points file\n",
    "pyemu.utils.write_pp_file(pp_file,df_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate the pilot point values to the grid\n",
    "hk_arr = pyemu.utils.fac2real(pp_file,factors_file=pp_file+\".fac\",out_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "ax = plt.subplot(111,aspect='equal')\n",
    "cb=ax.imshow(hk_arr,interpolation=\"nearest\",extent=m.sr.get_extent(),alpha=0.5)\n",
    "ax.scatter(df_pp.x,df_pp.y,marker='.',s=4,color='k')\n",
    "plt.colorbar(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
