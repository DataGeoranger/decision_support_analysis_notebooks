#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1cm
\topmargin 1.5cm
\rightmargin 1cm
\bottommargin 1.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
GW1876 Class Notes: Some Informal Background on the Gauss-Newton-Levenberg-Marqu
ardt Algorithm
\end_layout

\begin_layout Author
Mike Fienen
\end_layout

\begin_layout Section*
Linear Least Squares Regression
\end_layout

\begin_layout Standard
Starting with a linear relationship between measured and modeled values:
\begin_inset Formula 
\[
\mathbf{y}=\mathbf{X\beta}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathbf{y}$
\end_inset

 are the observations, 
\begin_inset Formula $\mathbf{\beta}$
\end_inset

 are the parameters, and 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is the Jacobian matrix of sensitivities, with 
\begin_inset Formula 
\[
X_{ij}=\frac{\partial y_{i}}{\partial b_{j}}
\]

\end_inset


\end_layout

\begin_layout Standard
In essence, 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is a codification of the relationship between observations and parameters
 in the model.
 In other words, for any set of parameters, one could multiply them by 
\begin_inset Formula $\mathbf{X}$
\end_inset

 and get an estimate of 
\begin_inset Formula $\mathbf{y}.$
\end_inset

 Flipping this in the inverse, we could estimate parameters from a set of
 observations as
\begin_inset Formula 
\[
\mathbf{\hat{\beta}=}\mathbf{X}^{-1}\mathbf{y}
\]

\end_inset


\end_layout

\begin_layout Standard
However, this relationship is seldom perfectly one-to-one.
 Typically there is noise which we can assume is normally distributed and
 with zero mean.
 If we represent the error by 
\begin_inset Formula $\mathbf{\epsilon}$
\end_inset

 we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{y}=\mathbf{X\beta}+\mathbf{\epsilon}
\]

\end_inset


\end_layout

\begin_layout Standard
Residuals are defined then as 
\begin_inset Formula 
\[
\mathbf{\epsilon=}\mathbf{y}-\mathbf{X\beta}
\]

\end_inset


\end_layout

\begin_layout Standard
The sum of squared errors is
\begin_inset Formula 
\[
\mathbf{\epsilon}^{T}\mathbf{\epsilon=}\left(\mathbf{y}\mathbf{-}\mathbf{X}\beta\right)^{T}\left(\mathbf{y}\mathbf{\mathbf{-}}\mathbf{X}\beta\right)
\]

\end_inset


\end_layout

\begin_layout Standard
So, like with any function, if we want to find the parameters that minimize
 it, we take the derivative with respect to the parameters 
\begin_inset Formula $\mathbf{\beta}$
\end_inset

 and set it to zero
\begin_inset Formula 
\[
\frac{\partial\mathbf{\epsilon}^{T}\mathbf{\epsilon}}{\partial\mathbf{\beta}^{T}}=\left(\mathbf{y}\mathbf{-}\mathbf{X}\beta\right)^{T}\left(\mathbf{y\mathbf{-}}\mathbf{X}\beta\right)=0
\]

\end_inset


\end_layout

\begin_layout Standard
multiplying out the terms
\begin_inset Formula 
\[
\left(\mathbf{y}\mathbf{-}\mathbf{X}\beta\right)^{T}\left(\mathbf{y}\mathbf{-}\mathbf{X}\beta\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{y}^{T}\mathbf{y}-\mathbf{\beta}^{T}\mathbf{X}^{T}\mathbf{y}-\mathbf{y}^{T}\mathbf{X\beta}+\mathbf{\beta}^{T}\mathbf{X}^{T}\mathbf{X}\mathbf{\beta}
\]

\end_inset


\end_layout

\begin_layout Standard
note that 
\begin_inset Formula $\mathbf{\beta}^{T}\mathbf{X}^{T}\mathbf{y}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}^{T}\mathbf{X\beta}$
\end_inset

 both collapse to a scalar and are, in fact, equivalent.
 So, we can simplify a bit
\begin_inset Formula 
\[
\mathbf{y}^{T}\mathbf{y}-2\mathbf{\beta}^{T}\mathbf{X}^{T}\mathbf{y}+\mathbf{\beta}^{T}\mathbf{X}^{T}\mathbf{X}\mathbf{\beta}
\]

\end_inset


\end_layout

\begin_layout Standard
Evaluating the derivative
\begin_inset Formula 
\[
\frac{\partial\left(\cancelto{0}{\mathbf{y}^{T}\mathbf{y}}-2\xcancel{\beta\mathbf{^{T}}}\mathbf{X}^{T}\mathbf{y}+\left(2\right)\xcancel{\beta\mathbf{^{T}}}\mathbf{X}^{T}\mathbf{X}\mathbf{\beta}\right)}{\partial\mathbf{\beta}^{T}}=0
\]

\end_inset


\begin_inset Formula 
\[
-2\mathbf{X}^{T}\mathbf{y}+2\mathbf{X}^{T}\mathbf{X}\mathbf{\beta}=0
\]

\end_inset


\begin_inset Formula 
\[
\cancelto{1}{2}\mathbf{X}^{T}\mathbf{X}\mathbf{\beta}=\cancelto{1}{2}\mathbf{X}^{T}\mathbf{y}
\]

\end_inset


\begin_inset Formula 
\[
\boxed{\hat{\mathbf{\beta}}=\left(\mathbf{X}^{T}\mathbf{X}\right)^{-1}\mathbf{X}^{T}\mathbf{y}}
\]

\end_inset


\end_layout

\begin_layout Standard
This can be solved in closed form if the problem is linear and is known
 as the 
\series bold
Newton solution
\series default
.
\end_layout

\begin_layout Section*
Adding Weights
\end_layout

\begin_layout Standard
Returning to the 
\begin_inset Formula $\mathbf{X}$
\end_inset

 matrix, we can think of the 
\begin_inset Formula $\mathbf{\hat{\beta}}$
\end_inset

 solution as a mapping from observations 
\begin_inset Formula $\mathbf{y}$
\end_inset

 to estimated parameters 
\begin_inset Formula $\mathbf{\hat{\beta}}$
\end_inset

, similar to in the noiseless case above 
\begin_inset Formula $\left(\mathbf{\hat{\beta}=}\mathbf{X}^{-1}\mathbf{y}\right)$
\end_inset

.
 The mapping is provided by 
\begin_inset Formula $\left(\mathbf{X}^{T}\mathbf{X}\right)^{-1}\mathbf{X}^{T}$
\end_inset

.
 The rows of 
\begin_inset Formula $\mathbf{X}$
\end_inset

 each contain the sensitivity of of a single observation to all the parameters.
 So, in a sense, each parameter in 
\begin_inset Formula $\mathbf{\hat{\beta}}$
\end_inset

 comprises a weighted product of all the observations.
 This is powerful in that it formally shows that the estimates of 
\begin_inset Formula $\mathbf{\hat{\beta}}$
\end_inset

 are directly the result of both the observations used and the modeled sensitivi
ty between parameters and the observations.
 
\end_layout

\begin_layout Standard
However, this also puts signficant faith in each observation used.
 In reality, observations have errors and are not equally informative.
 So, we can provide a matrix of weights, often but not necessarily diagonal,
 defined as 
\begin_inset Formula $\mathbf{Q.}$
\end_inset

 This matrix is square, with dimensions of 
\begin_inset Formula $NOBS\times NOBS$
\end_inset

.
 This gets incorporated into the sum of squared errors formulation, replacing
\begin_inset Formula 
\[
\mathbf{\epsilon}^{T}\mathbf{\epsilon=}\left(\mathbf{y}\mathbf{-}\mathbf{X}\beta\right)^{T}\left(\mathbf{y}\mathbf{\mathbf{-}}\mathbf{X}\beta\right)
\]

\end_inset


\end_layout

\begin_layout Standard
with 
\begin_inset Formula 
\[
\mathbf{\Phi=}\left(\mathbf{y}\mathbf{-}\mathbf{X}\beta\right)^{T}\mathbf{Q}\left(\mathbf{y}\mathbf{\mathbf{\mathbf{-}}X}\beta\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Multiplying this out:
\begin_inset Formula 
\[
\mathbf{\Phi=}\left(\mathbf{y}\mathbf{-}X\beta\right)^{T}\mathbf{Q}^{1/2}\mathbf{Q}^{1/2}\left(\mathbf{y}\mathbf{\mathbf{-}}X\beta\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{y}^{T}\mathbf{Qy}-\mathbf{\beta}^{T}\mathbf{X}^{T}\mathbf{Qy}-\mathbf{y}^{T}\mathbf{QX\beta}+\mathbf{\beta}^{T}\mathbf{X}^{T}\mathbf{QX}\mathbf{\beta}
\]

\end_inset


\end_layout

\begin_layout Standard
Again we can simplify since 
\begin_inset Formula $\mathbf{\beta}^{T}\mathbf{X}^{T}\mathbf{Qy}$
\end_inset

 still multiplies out to a scalar
\begin_inset Formula 
\[
\mathbf{y}^{T}\mathbf{Q}\mathbf{y}-2\mathbf{\beta}^{T}\mathbf{X}^{T}\mathbf{\mathbf{Q}y}+\mathbf{\beta}^{T}\mathbf{X}^{T}\mathbf{Q}\mathbf{X}\mathbf{\beta}
\]

\end_inset


\end_layout

\begin_layout Standard
Evaluate the derivative
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial\left(\cancelto{0}{\mathbf{y}^{T}\mathbf{Q}\mathbf{y}}-2\xcancel{\beta\mathbf{^{T}}}\mathbf{X}^{T}\mathbf{Q}\mathbf{y}+\left(2\right)\xcancel{\beta\mathbf{^{T}}}\mathbf{X}^{T}\mathbf{QX}\mathbf{\beta}\right)}{\partial\mathbf{\beta}^{T}}=0
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
-2\mathbf{X}^{T}\mathbf{Q}\mathbf{y}+2\mathbf{X}^{T}\mathbf{QX}\mathbf{\beta}=0
\]

\end_inset


\begin_inset Formula 
\[
\cancelto{1}{2}\mathbf{X}^{T}\mathbf{Q}\mathbf{X}\mathbf{\beta}=\cancelto{1}{2}\mathbf{X}^{T}\mathbf{Qy}
\]

\end_inset


\begin_inset Formula 
\[
\boxed{\hat{\mathbf{\beta}}=\left(\mathbf{X}^{T}\mathbf{QX}\right)^{-1}\mathbf{X}^{T}\mathbf{Qy}}
\]

\end_inset


\end_layout

\begin_layout Section*
Nonlinear Least-Squaresâ€”the Gauss-Newton Algorithm
\end_layout

\begin_layout Standard
If the relationship, continuing in the weighted case 
\begin_inset Formula $\mathbf{y}=\mathbf{X}\mathbf{Q}\mathbf{\beta}+\mathbf{\epsilon}$
\end_inset

 is not linear, it gets expressed as 
\begin_inset Formula 
\[
\mathbf{y}=f\left(\mathbf{X}\mathbf{Q},\mathbf{\beta}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
This can get expanded in a Taylor series to linearize it and then iterate
 using similar equations
\begin_inset Formula 
\[
\boxed{\mathbf{\hat{\beta}}_{i+1}=\mathbf{\hat{\beta}}_{i}+\left(\mathbf{X}^{T}\mathbf{QX}\right)^{-1}\mathbf{X}^{T}\mathbf{Q}\mathbf{\left(\mathbf{y}-\mathbf{X\hat{\beta_{i}}}\right)}}
\]

\end_inset


\end_layout

\begin_layout Standard
This keeps iterating from some initial estimate until the difference between
 subsequent estimates of 
\begin_inset Formula $\hat{\beta}$
\end_inset

 decreases to within a tolerance or when 
\begin_inset Formula $\mathbf{X}$
\end_inset

, which is calcualted as a function of 
\begin_inset Formula $\hat{\beta}_{i}$
\end_inset

, changes less than a tolerance from one iteration to another.
\end_layout

\begin_layout Section*
The Levenberg-Marquardt Adjustment
\end_layout

\begin_layout Standard
Given the relationship
\begin_inset Formula 
\[
\boxed{\mathbf{\hat{\beta}}_{i+1}=\mathbf{\hat{\beta}}_{i}+\left(\mathbf{X}^{T}\mathbf{QX}\right)^{-1}\mathbf{X}^{T}\mathbf{Q}\mathbf{\left(\mathbf{y}-\mathbf{X\hat{\beta_{i}}}\right)}}
\]

\end_inset


\end_layout

\begin_layout Standard
Levenberg and Marquardt, separately, found an improvement that can help
 the algorithm progress.
 As it happents, the Newton direction is not always optimal as it progresses
 through parameter space 
\begin_inset Quotes eld
\end_inset

downhill
\begin_inset Quotes erd
\end_inset

 toward an optimum.
 An option is to form a trust region around the current parameters and assume
 the problem is linear in that trust region.
 At the extreme, if the trust region is really large, then the steepest-gradient
 direction is used.
 If the trust region is small, the Newton correction to the steepest-descent
 gets used.
 
\end_layout

\begin_layout Standard
The Levenberg approach implements this by adding a weighted diagional (identity)
 matrix to the normal equations like
\begin_inset Formula 
\[
\boxed{\mathbf{\hat{\beta}}_{i+1}=\mathbf{\hat{\beta}}_{i}+\left(\mathbf{X}^{T}\mathbf{QX}+\lambda\mathbf{I}\right)^{-1}\mathbf{X}^{T}\mathbf{Q}\left(\mathbf{y}-\mathbf{X\hat{\beta_{i}}}\right)}
\]

\end_inset


\end_layout

\begin_layout Standard
The Marquardt adaptation to Levenberg's method replaces 
\begin_inset Formula $\mathbf{I}$
\end_inset

 with the diagonal of 
\begin_inset Formula $\mathbf{X}^{T}\mathbf{X}$
\end_inset

 as 
\begin_inset Formula 
\[
\boxed{\mathbf{\hat{\beta}}_{i+1}=\mathbf{\hat{\beta}}_{i}+\left(\mathbf{X}^{T}\mathbf{QX}+\lambda diag\left(\mathbf{X}^{T}\mathbf{X}\right)\right)^{-1}\mathbf{X}^{T}\mathbf{Q}\left(\mathbf{y}-\mathbf{X\hat{\beta_{i}}}\right)}
\]

\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $\lambda$
\end_inset

 is small, very little adjustment from the Newton direction takes place.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename response_surface_nolam.png

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
If 
\begin_inset Formula $\lambda$
\end_inset

 is moderate, the descent direction moves toward steepest descent somewhat.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename response_surface_lam1.0.png

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
However, if 
\begin_inset Formula $\lambda$
\end_inset

 is large, then 
\begin_inset Formula $\left(\mathbf{X}^{T}\mathbf{QX}+\lambda diag\left(\mathbf{X}^{T}\mathbf{X}\right)\right)^{-1}\rightarrow\left(\xcancel{\mathbf{X}^{T}\mathbf{QX}+}\lambda diag\left(\mathbf{X}^{T}\mathbf{X}\right)\right)^{-1}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename response_surface_lam50.0.png

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A second impact of this is scaling the steepest descent direction 
\begin_inset Formula $\mathbf{X}^{T}$
\end_inset

.
 If 
\begin_inset Formula $\lambda$
\end_inset

 is small, again, the step size implied by 
\begin_inset Formula $\left(\mathbf{X}^{T}\mathbf{QX}+\lambda diag\left(\mathbf{X}^{T}\mathbf{X}\right)\right)^{-1}\mathbf{X}^{T}\mathbf{Q}$
\end_inset

 doesn't change much, but if 
\begin_inset Formula $\lambda$
\end_inset

 is large, 
\begin_inset Formula $\left(\lambda diag\left(\mathbf{X}^{T}\mathbf{X}\right)\right)^{-1}\mathbf{X}^{T}\mathbf{Q}$
\end_inset

 behaves like dividing the gradient by 
\begin_inset Formula $\lambda.$
\end_inset

 This decreases the step size.
 There is an advantage to this over unaltered steepest descent in that overshoot
ing (hemstitching) is decreased.
\end_layout

\begin_layout Standard
In practice, 
\begin_inset Formula $\lambda$
\end_inset

 is helpful, but it's difficult to know what value is appropriate.
 PEST and PEST++ try multiple values at each iteration and, in each case,
 choose the value of 
\begin_inset Formula $\lambda$
\end_inset

 that reduces 
\begin_inset Formula $\mathbf{\Phi}$
\end_inset

 the most.
\end_layout

\end_body
\end_document
